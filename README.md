# toxic-comments-detector

## Структура проекта
```
├── data # Используемые данные
│
├── dev # Набор констант и утилит для разработки
│   ├── constants
|   └── utils
│
├── models # различные модели для решения главной задачи классификации  
│    
├── textPreprocessing # набор утилит для предобработки русскоязычных текстов
│   ├── text_utils # набор маленьких утилит по очистке текста
│   └── preprocess_text.py # собирает все утилиты из text_utils в одну функцию
│
└── wordEmbeddingsLayers # различные способы векторизации слов
```

## Навигация по проекту
- [Используемые данные](/data)
- [Предварительная очистка текстовых данных](/textPreprocessing)<br>
Как обрабатывались сырые текстовые данные перед их дальнейшим использованием в построении моделей машинного обучения.
- [Какие способы векторизации слов применялись](/wordEmbeddingsLayers)
- [Какие модели ML были опробованы для решения поставленной главной задачи классификации](/models)


## TODO
- [X] Добавить weights (токсичных комментов меньше)
- [X] Сделать единую утилиту evaluate_model для всех блокнотов с моделями (добавить туда графиков, использующих history)
- [X] Попробовать [navec](https://github.com/natasha/navec)
- [X] Попробовать ConvNet
- [X] Попробовать pretrained FastText
- [ ] Попробовать натренировать свой FastText через gensim
- [ ] Попробовать BERT